---
title: "ME114 Day 4: Solutions for Applied Exercises on Classification and Non-linear models"
author: "Ken Benoit and Slava Mikhaylov"
output: html_document
bibliography: ../bibliography/bibliography.bib
---

Detailed description of the exercises is provided in @james2013. Part I below covers solutions to applied exercises for Chapter 4 "Classification" [pp. 171-173] and Part II solutions for applied exercises for Chapter 7 "Moving Beyond Linearity" [pp. 299-301].

Solutions provided here are based on @weatherwax2014. 

## Part I: Classification

### Exercise 4.1

```{r}
save_plots <- F

library("ISLR")
library("MASS")
library("class")

set.seed(0)

# Part (a):
Direction <- Weekly$Direction
Weekly$Direction <- NULL
Weekly$NumericDirection <- as.numeric(Direction)  # Maps Down=>1 and Up=>2
Weekly$NumericDirection[Weekly$NumericDirection == 1] <- -1  # Maps Down=>-1 and Up=>2
Weekly$NumericDirection[Weekly$NumericDirection == 2] <- +1  # Maps Down=>-1 and Up=>+1

# Look at the correlation between the output and the input lags:
Weekly.cor <- cor(Weekly)

# b: logistic regression to predict Direction as a function of 5 lag variables + volume:
Weekly$NumericDirection <- NULL
Weekly$Direction <- Direction

five_lag_model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(five_lag_model)

contrasts(Weekly$Direction)

# c: the confusion matrix:
p_hat <- predict(five_lag_model, newdata = Weekly, type = "response")
y_hat <- rep("Down", length(p_hat))
y_hat[p_hat > 0.5] <- "Up"
CM <- table(predicted = y_hat, truth = Weekly$Direction)
CM
sprintf("LR (all features): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))


# d: logistic regression using only Lag2 as the predictor (since it is the most significant predictor)
Weekly.train <- (Weekly$Year >= 1990) & (Weekly$Year <= 2008)  # our training set 
Weekly.test <- (Weekly$Year >= 2009)  # our testing set 
lag2_model <- glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = Weekly.train)

# CM on test data :
p_hat <- predict(lag2_model, newdata = Weekly[Weekly.test, ], type = "response")
y_hat <- rep("Down", length(p_hat))
y_hat[p_hat > 0.5] <- "Up"
CM <- table(predicted = y_hat, truth = Weekly[Weekly.test, ]$Direction)
CM
sprintf("LR (only Lag2): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

# e: Use LDA
lda.fit <- lda(Direction ~ Lag2, data = Weekly, subset = Weekly.train)

lda.predict <- predict(lda.fit, newdata = Weekly[Weekly.test, ])
CM <- table(predicted = lda.predict$class, truth = Weekly[Weekly.test, ]$Direction)
CM
sprintf("LDA (only Lag2): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))


# f: Use QDA
qda.fit <- qda(Direction ~ Lag2, data = Weekly, subset = Weekly.train)

qda.predict <- predict(qda.fit, newdata = Weekly[Weekly.test, ])
CM <- table(predicted = qda.predict$class, truth = Weekly[Weekly.test, ]$Direction)
CM
sprintf("QDA (only Lag2): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

# g: KNN
X.train <- data.frame(Lag2 = Weekly[Weekly.train, ]$Lag2)
Y.train <- Weekly[Weekly.train, ]$Direction

X.test <- data.frame(Lag2 = Weekly[Weekly.test, ]$Lag2)

y_hat_k_1 <- knn(X.train, X.test, Y.train, k = 1)

CM <- table(predicted = y_hat_k_1, truth = Weekly[Weekly.test, ]$Direction)
CM
sprintf("KNN (k=1): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

y_hat_k_3 <- knn(X.train, X.test, Y.train, k = 3)
CM <- table(predicted = y_hat_k_3, truth = Weekly[Weekly.test, ]$Direction)
CM
sprintf("KNN (k=1): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))
```


### Exercise 4.2

```{r}
attach(Auto)

set.seed(0)

# Part (a):
mpg01 <- rep(0, dim(Auto)[1])  # 0 => less than the median of mpg 
mpg01[Auto$mpg > median(Auto$mpg)] <- 1  # 1 => greater than the median of mpg 

Auto$mpg01 <- mpg01
Auto$mpg <- NULL

# Part (b):
pairs(Auto)

Auto$mpg01 <- as.factor(mpg01)

# Part (c):
n <- dim(Auto)[1]
inds.train <- sample(1:n, 3 * n/4)
Auto.train <- Auto[inds.train, ]
inds.test <- (1:n)[-inds.train]
Auto.test <- Auto[inds.test, ]

# Part (d) Use LDA:
lda.fit <- lda(mpg01 ~ cylinders + displacement + weight, data = Auto.train)

lda.predict <- predict(lda.fit, newdata = Auto.test)
CM <- table(predicted = lda.predict$class, truth = Auto.test$mpg01)
CM
sprintf("LDA: overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

# Part (e): Use QDA:
qda.fit <- qda(mpg01 ~ cylinders + displacement + weight, data = Auto.train)

qda.predict <- predict(qda.fit, newdata = Auto.test)
CM <- table(predicted = qda.predict$class, truth = Auto.test$mpg01)
CM
sprintf("QDA: overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

# Part (f): Use Logistic regression:
lr.fit <- glm(mpg01 ~ cylinders + displacement + weight, data = Auto.train, family = binomial)

p_hat <- predict(lr.fit, newdata = Auto.test, type = "response")
y_hat <- rep(0, length(p_hat))
y_hat[p_hat > 0.5] <- 1
CM <- table(predicted = as.factor(y_hat), truth = Auto.test$mpg01)
CM
sprintf("LR (all features): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

```

### Exercise 4.3

```{r}
# Part (a):
Power <- function() {
    print(2^3)
}

# Part (b):
Power2 <- function(x, a) {
    print(x^a)
}
```

### Exercise 4.4

```{r}
set.seed(0)

n <- dim(Boston)[1]

# Introduce a variable whether or not the crime rate is above=1 / below=0 the median
Boston$crim01 <- rep(0, n)
Boston$crim01[Boston$crim >= median(Boston$crim)] <- 1
Boston$crim <- NULL

# Look to see what features are most strongly correlated with crim01:
Boston.cor <- cor(Boston)
print(sort(Boston.cor[, "crim01"]))

# Split the data set into testing and training parts:
inds.train <- sample(1:n, 3 * n/4)
inds.test <- (1:n)[-inds.train]
Boston.train <- Boston[inds.train, ]
Boston.test <- Boston[inds.test, ]

# Fit several models to the training data
lr_model <- glm(crim01 ~ nox + rad + dis, data = Boston.train, family = binomial)

p_hat <- predict(lr_model, newdata = Boston.test, type = "response")
y_hat <- rep(0, length(p_hat))
y_hat[p_hat > 0.5] <- 1
CM <- table(predicted = y_hat, truth = Boston.test$crim01)
CM
sprintf("LR: overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

lda.fit <- lda(crim01 ~ nox + rad + dis, data = Boston.train)

# Use LDA
lda.fit <- lda(crim01 ~ nox + rad + dis, data = Boston.train)

lda.predict <- predict(lda.fit, newdata = Boston.test)
CM <- table(predicted = lda.predict$class, truth = Boston.test$crim01)
CM
sprintf("LDA: overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

# f: Use QDA
qda.fit <- qda(crim01 ~ nox + rad + dis, data = Boston.train)

qda.predict <- predict(qda.fit, newdata = Boston.test)
CM <- table(predicted = qda.predict$class, truth = Boston.test$crim01)
CM
sprintf("QDA: overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

# g: KNN
X.train <- Boston.train
X.train$crim01 <- NULL
Y.train <- Boston.train$crim01

X.test <- Boston.test
X.test$crim01 <- NULL

y_hat_k_1 <- knn(X.train, X.test, Y.train, k = 1)

CM <- table(predicted = y_hat_k_1, truth = Boston.test$crim01)
CM
sprintf("KNN (k=1): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))

y_hat_k_3 <- knn(X.train, X.test, Y.train, k = 3)
CM <- table(predicted = y_hat_k_3, truth = Boston.test$crim01)
CM
sprintf("KNN (k=3): overall fraction correct= %10.6f", (CM[1, 1] + CM[2, 2])/sum(CM))
```


## Part II: Moving Beyond Linearity

### Exercise 4.5

```{r}
library(boot)
library(ISLR)
attach(Wage)

set.seed(0)

# Plot the data to see what it looks like:
with(Wage, plot(age, wage))


# Part (a):

# Perform polynomial regression for various polynomial degrees:
cv.error <- rep(0, 10)
for (i in 1:10) {
    # fit polynomial models of various degrees
    glm.fit <- glm(wage ~ poly(age, i), data = Wage)
    cv.error[i] <- cv.glm(Wage, glm.fit, K = 10)$delta[1]
}

plot(1:10, cv.error, pch = 19, type = "b", xlab = "degree of polynomial", ylab = "CV estimate of the prediction error")
grid()

# Using the minimal value for the CV error gives the value 10 which seems like too much polynomial i.e. too wiggly From the plot 4 is
# the point where the curve stops decreasing and starts increasing so we will consider polynomials of this degree.
me <- which.min(cv.error)
me <- 4

m <- glm(wage ~ poly(age, me), data = Wage)

plot(Wage$age, Wage$wage)

aRng <- range(Wage$age)

a_predict <- seq(from = aRng[1], to = aRng[2], length.out = 100)
w_predict <- predict(m, newdata = list(age = a_predict))
lines(a_predict, w_predict, col = "red")

# Lets consider the ANOVA approach (i.e. a sequence of nested linear models):
m0 <- lm(wage ~ 1, data = Wage)
m1 <- lm(wage ~ poly(age, 1), data = Wage)
m2 <- lm(wage ~ poly(age, 2), data = Wage)
m3 <- lm(wage ~ poly(age, 3), data = Wage)
m4 <- lm(wage ~ poly(age, 4), data = Wage)
m5 <- lm(wage ~ poly(age, 5), data = Wage)
anova(m0, m1, m2, m3, m4, m5)


# Part (b):

# We will do cross-validation by hand to understand what's going on.  
number_of_bins <- c(2, 3, 4, 5, 10)
nc <- length(number_of_bins)

k <- 10
folds <- sample(1:k, nrow(Wage), replace = TRUE)
cv.errors <- matrix(NA, k, nc)

# Prepare for the type of factors you might obtain (extend the age range a bit):
age_range <- range(Wage$age)
age_range[1] <- age_range[1] - 1
age_range[2] <- age_range[2] + 1

for (ci in 1:nc) {
    # for each number of cuts to test
    nob <- number_of_bins[ci]  # n(umber) o(f) c(uts) 2, 3, 4 ...
    
    for (fi in 1:k) {
        # for each fold
        
        # In this ugly command we: break the 'age' variable in the subset of data Wage[folds!=fi,] into 'nob' bins that span between the
        # smallest and largest values of age observed over the entire dataset.  This allows us to be able to use the function 'predict' on
        # age values not seen in the training subset.  If we try to 'cut' the age variable into bins that are too small they may not contain
        # any ages in them.  Estimations with lm/glm in that case may not perform optimally in that case. Thus we only do cross-validation on a
        # smallish number of bins.
        fit <- glm(wage ~ cut(age, breaks = seq(from = age_range[1], to = age_range[2], length.out = (nob + 1))), data = Wage[folds != 
            fi, ])
        y_hat <- predict(fit, newdata = Wage[folds == fi, ])
        cv.errors[fi, ci] <- mean((Wage[folds == fi, ]$wage - y_hat)^2)
    }
    
}

cv.errors.mean <- apply(cv.errors, 2, mean)
cv.errors.stderr <- apply(cv.errors, 2, sd)/sqrt(k)

min.cv.index <- which.min(cv.errors.mean)
one_se_up_value <- (cv.errors.mean + cv.errors.stderr)[min.cv.index]

plot(number_of_bins, cv.errors.mean, pch = 19, type = "b", xlab = "number of cut bins", ylab = "CV estimate of the prediction error")
lines(number_of_bins, cv.errors.mean - cv.errors.stderr, lty = "dashed")
lines(number_of_bins, cv.errors.mean + cv.errors.stderr, lty = "dashed")
abline(h = one_se_up_value, col = "red")
grid()

# Fit the optimal model using all data:
nob <- 3
fit <- glm(wage ~ cut(age, breaks = seq(from = age_range[1], to = age_range[2], length.out = (nob + 1))), data = Wage)

plot(Wage$age, Wage$wage)

aRng <- range(Wage$age)

a_predict <- seq(from = aRng[1], to = aRng[2], length.out = 100)
w_predict <- predict(fit, newdata = list(age = a_predict))
lines(a_predict, w_predict, col = "red", lw = 4)
```

### Exercise 4.6

```{r}
library(MASS)
library(splines)

set.seed(0)

# Part (a):
m <- lm(nox ~ poly(dis, 3), data = Boston)

plot(Boston$dis, Boston$nox, xlab = "dis", ylab = "nox", main = "third degree polynomial fit")

dis_range <- range(Boston$dis)
dis_samples <- seq(from = dis_range[1], to = dis_range[2], length.out = 100)
y_hat <- predict(m, newdata = list(dis = dis_samples))

lines(dis_samples, y_hat, col = "red")
grid()


# Part (b-c):
d_max <- 10

# The training RSS:
training_rss <- rep(NA, d_max)
for (d in 1:d_max) {
    m <- lm(nox ~ poly(dis, d), data = Boston)
    training_rss[d] <- sum((m$residuals)^2)
}

# The RSS estimated using cross-validation:
k <- 10
folds <- sample(1:k, nrow(Boston), replace = TRUE)
cv.rss.test <- matrix(NA, k, d_max)
cv.rss.train <- matrix(NA, k, d_max)

for (d in 1:d_max) {
    for (fi in 1:k) {
        # for each fold
        fit <- lm(nox ~ poly(dis, d), data = Boston[folds != fi, ])
        
        y_hat <- predict(fit, newdata = Boston[folds != fi, ])
        cv.rss.train[fi, d] <- sum((Boston[folds != fi, ]$nox - y_hat)^2)
        
        y_hat <- predict(fit, newdata = Boston[folds == fi, ])
        cv.rss.test[fi, d] <- sum((Boston[folds == fi, ]$nox - y_hat)^2)
    }
}

cv.rss.train.mean <- apply(cv.rss.train, 2, mean)
cv.rss.train.stderr <- apply(cv.rss.train, 2, sd)/sqrt(k)

cv.rss.test.mean <- apply(cv.rss.test, 2, mean)
cv.rss.test.stderr <- apply(cv.rss.test, 2, sd)/sqrt(k)

min_value <- min(c(cv.rss.test.mean, cv.rss.train.mean))
max_value <- max(c(cv.rss.test.mean, cv.rss.train.mean))

plot(1:d_max, cv.rss.train.mean, xlab = "polynomial degree", ylab = "RSS", col = "red", pch = 19, type = "b", ylim = c(min_value, max_value))
lines(1:d_max, cv.rss.test.mean, col = "green", pch = 19, type = "b")
grid()
legend("topright", legend = c("train RSS", "test RSS"), col = c("red", "green"), lty = 1, lwd = 2)

# Part (d-f):
m <- lm(nox ~ bs(dis, df = 4), data = Boston)

plot(Boston$dis, Boston$nox, xlab = "dis", ylab = "nox", main = "bs with df=4 fit")

dis_range <- range(Boston$dis)
dis_samples <- seq(from = dis_range[1], to = dis_range[2], length.out = 100)
y_hat <- predict(m, newdata = list(dis = dis_samples))

lines(dis_samples, y_hat, col = "red")
grid()


dof_choices <- c(3, 4, 5, 10, 15, 20)
n_dof_choices <- length(dof_choices)

# The RSS estimated using cross-validation:
k <- 5
folds <- sample(1:k, nrow(Boston), replace = TRUE)
cv.rss.test <- matrix(NA, k, n_dof_choices)
cv.rss.train <- matrix(NA, k, n_dof_choices)

for (di in 1:n_dof_choices) {
    for (fi in 1:k) {
        # for each fold
        fit <- lm(nox ~ bs(dis, df = dof_choices[di]), data = Boston[folds != fi, ])
        
        y_hat <- predict(fit, newdata = Boston[folds != fi, ])
        cv.rss.train[fi, di] <- sum((Boston[folds != fi, ]$nox - y_hat)^2)
        
        y_hat <- predict(fit, newdata = Boston[folds == fi, ])
        cv.rss.test[fi, di] <- sum((Boston[folds == fi, ]$nox - y_hat)^2)
    }
}

cv.rss.train.mean <- apply(cv.rss.train, 2, mean)
cv.rss.train.stderr <- apply(cv.rss.train, 2, sd)/sqrt(k)

cv.rss.test.mean <- apply(cv.rss.test, 2, mean)
cv.rss.test.stderr <- apply(cv.rss.test, 2, sd)/sqrt(k)

min_value <- min(c(cv.rss.test.mean, cv.rss.train.mean))
max_value <- max(c(cv.rss.test.mean, cv.rss.train.mean))

plot(dof_choices, cv.rss.train.mean, xlab = "spline d.o.f.", ylab = "RSS", col = "red", pch = 19, type = "b", ylim = c(min_value, max_value))
lines(dof_choices, cv.rss.test.mean, col = "green", pch = 19, type = "b")
grid()
legend("topright", legend = c("train RSS", "test RSS"), col = c("red", "green"), lty = 1, lwd = 2)

```


### Exercise 4.7

```{r}
library("leaps")
library("glmnet")
library("gam")

set.seed(0)

# Part (a):

# Divide the dataset into three parts: training==1, validation==2, and test==3
dataset_part <- sample(1:3, nrow(College), replace = T, prob = c(0.5, 0.25, 0.25))

p <- ncol(College) - 1

# Fit subsets of various sizes:
regfit.forward <- regsubsets(Outstate ~ ., data = College[dataset_part == 1, ], nvmax = p, method = "forward")
print(summary(regfit.forward))

reg.summary <- summary(regfit.forward)

# Test the trained models on the validation set:
validation.mat <- model.matrix(Outstate ~ ., data = College[dataset_part == 2, ])
val.errors <- rep(NA, p)
for (ii in 1:p) {
    coefi <- coef(regfit.forward, id = ii)
    pred <- validation.mat[, names(coefi)] %*% coefi
    val.errors[ii] <- mean((College$Outstate[dataset_part == 2] - pred)^2)
}
```

forward selection validation errors `r val.errors`

```{r}
k <- which.min(val.errors)
```

smallest validation error for index= `r k`, with coefficients given by:

```{r}
coef(regfit.forward, id = k)

plot(val.errors, xlab = "Number of variables", ylab = "Validation MSE", pch = 19, type = "b")
abline(v = k, col = "red")
grid()

# Predict the best model found on the testing set:
test.mat <- model.matrix(Outstate ~ ., data = College[dataset_part == 3, ])
coefi <- coef(regfit.forward, id = k)
pred <- test.mat[, names(coefi)] %*% coefi
test.error <- mean((College$Outstate[dataset_part == 3] - pred)^2)
```

test error on the optimal subset `{r test.error}`

```{r}
k <- 3
coefi <- coef(regfit.forward, id = k)
pred <- test.mat[, names(coefi)] %*% coefi
test.error <- mean((College$Outstate[dataset_part == 3] - pred)^2)
```

test error on k=3 subset `{r test.error}`

```{r}
# Part (b):

# Combine the training and validation into one 'training' dataset
dataset_part[dataset_part == 2] <- 1
dataset_part[dataset_part == 3] <- 2

gam.model <- gam(Outstate ~ s(Expend, 4) + s(Room.Board, 4) + Private, data = College[dataset_part == 1, ])

par(mfrow = c(1, 3))
plot(gam.model, se = TRUE, col = "blue")
par(mfrow = c(1, 1))

# Predict the GAM performance on the test dataset:
y_hat <- predict(gam.model, newdata = College[dataset_part == 2, ])
MSE <- mean((College[dataset_part == 2, ]$Outstate - y_hat)^2)
```

gam testing set (MSE) error `r MSE`


## References

